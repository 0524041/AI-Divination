"""
AI æœå‹™æ¨¡çµ„
"""

import httpx
import logging
import asyncio
import random
import json
from typing import Optional, AsyncGenerator, Any, Callable
from app.core.config import get_settings
from app.utils.auth import decrypt_api_key
from google import genai
from google.genai import types

from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

settings = get_settings()


class AIService:
    """AI æœå‹™åŸºé¡"""

    async def generate(self, prompt: str, system_prompt: str) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        raise NotImplementedError

    async def generate_stream(
        self, prompt: str, system_prompt: str
    ) -> AsyncGenerator[str, None]:
        """ä¸²æµç”Ÿæˆå›æ‡‰"""
        raise NotImplementedError


class GeminiService(AIService):
    """Google Gemini AI æœå‹™"""

    def __init__(self, api_key: str, model: str = "gemini-3-flash-preview"):
        self.api_key = api_key
        self.model = model or "gemini-3-flash-preview"
        self.client = genai.Client(
            api_key=api_key, http_options={"api_version": "v1alpha"}
        )

    async def _retry_async(
        self,
        func: Callable,
        *args,
        max_retries: int = 3,
        base_delay: float = 2.0,
        **kwargs,
    ) -> Any:
        """éåŒæ­¥é‡è©¦é‚è¼¯"""
        for attempt in range(max_retries):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                error_str = str(e)
                is_429 = "429" in error_str or "RESOURCE_EXHAUSTED" in error_str
                is_503 = "503" in error_str or "Service Unavailable" in error_str

                # ğŸ”´ 429 éŒ¯èª¤ï¼ˆé…é¡ç”¨å®Œï¼‰ç«‹å³çµ‚æ­¢ï¼Œä¸é‡è©¦
                if is_429:
                    logger.error(f"Gemini API é…é¡å·²ç”¨å®Œ (429 RESOURCE_EXHAUSTED)")
                    raise Exception(
                        "âš ï¸ Gemini API é…é¡å·²ç”¨å®Œï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æª¢æŸ¥ API é…é¡é™åˆ¶"
                    ) from e

                # å¦‚æœæ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ‹‹å‡ºå‹å–„éŒ¯èª¤è¨Šæ¯
                if attempt == max_retries - 1:
                    if is_503:
                        logger.error(
                            f"Gemini API æœå‹™ä¸å¯ç”¨ (503 Service Unavailable) - å·²é‡è©¦ {max_retries} æ¬¡"
                        )
                        raise Exception(
                            "âš ï¸ Gemini API æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œå·²é‡è©¦å¤šæ¬¡ä»å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦"
                        ) from e
                    else:
                        logger.error(
                            f"Gemini API Error (Attempt {attempt + 1}/{max_retries}): {error_str}"
                        )
                        raise e

                # é€²è¡Œé‡è©¦ï¼ˆåŒ…å« 503 éŒ¯èª¤ï¼‰
                sleep_time = base_delay * (2**attempt) + random.uniform(0, 1)
                error_type = "503 Service Unavailable" if is_503 else "API Error"
                logger.warning(
                    f"Gemini {error_type}. Retrying in {sleep_time:.2f}s... (Attempt {attempt + 1}/{max_retries})"
                )
                await asyncio.sleep(sleep_time)

    async def generate(self, prompt: str, system_prompt: str) -> str:
        """ç”Ÿæˆå›æ‡‰ (ä½¿ç”¨ Thinking Config)"""
        config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            temperature=1.0,
            max_output_tokens=16384,
        )

        try:
            response = await self._retry_async(
                self.client.aio.models.generate_content,
                model=self.model,
                contents=prompt,
                config=config,
            )
            return response.text
        except Exception as e:
            logger.error(f"Error in Gemini generate: {e}")
            raise e


class OpenAIService(AIService):
    """OpenAI å®˜æ–¹æœå‹™ (ä½¿ç”¨ SDK)"""

    def __init__(self, api_key: str, model: str = "gpt-5.1"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model or "gpt-5.1"  # é è¨­ gpt-5.1, ä½†å…è¨±ç”¨æˆ¶è‡ªå®šç¾©

    async def generate(self, prompt: str, system_prompt: str) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=1.0,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in OpenAI generate: {e}")
            raise e


class CustomAIService(AIService):
    """å…¶ä»– AI æœå‹™ (OpenAI Compatible)"""

    def __init__(self, base_url: str, model: str, api_key: Optional[str] = None):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.api_key = api_key

    async def generate(self, prompt: str, system_prompt: str) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        url = f"{self.base_url}/v1/chat/completions"

        headers = {}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            "temperature": 1.0,
            "max_tokens": 16384,
        }

        async with httpx.AsyncClient(timeout=300.0) as client:
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()

            if "choices" in data and len(data["choices"]) > 0:
                return data["choices"][0]["message"]["content"]
            return ""

    @staticmethod
    async def test_connection(base_url: str) -> dict:
        """
        æ¸¬è©¦é€£ç·š (å®‰å…¨å¼·åŒ–ç‰ˆ)
        é˜²è­·æªæ–½:
        1. ç¦æ­¢è·Ÿéš¨é‡å®šå‘ (é˜²æ­¢ç¹é SSRF æª¢æŸ¥)
        2. æª¢æŸ¥ Content-Type (é˜²æ­¢ä¸‹è¼‰äºŒé€²åˆ¶æª”æ¡ˆ)
        3. é™åˆ¶å›æ‡‰å¤§å° (é˜²æ­¢ DoS)
        """
        MAX_SIZE = 1024 * 50  # é™åˆ¶è®€å– 50KB

        async def fetch_safely(client, url):
            try:
                # ä½¿ç”¨ stream=True é¿å…ç›´æ¥ä¸‹è¼‰å¤§æª”æ¡ˆ
                async with client.stream(
                    "GET", url, follow_redirects=False
                ) as response:
                    # 1. æª¢æŸ¥ç‹€æ…‹ç¢¼
                    if response.status_code != 200:
                        return None

                    # 2. æª¢æŸ¥ Content-Type
                    content_type = response.headers.get("content-type", "").lower()
                    if "application/json" not in content_type:
                        logger.warning(
                            f"Blocked non-JSON response from {url}: {content_type}"
                        )
                        return None

                    # 3. è®€å–é™åˆ¶å¤§å°çš„å…§å®¹
                    content = b""
                    async for chunk in response.aiter_bytes():
                        content += chunk
                        if len(content) > MAX_SIZE:
                            break

                    return content
            except Exception as e:
                logger.warning(f"Connection test failed for {url}: {e}")
                return None

        try:
            # è¨­ç½®è¼ƒçŸ­çš„ timeout
            async with httpx.AsyncClient(timeout=5.0) as client:
                # å„ªå…ˆå˜—è©¦: OpenAI Compatible (/v1/models) - æ”¯æ´ LM Studio, vLLM ç­‰
                url_openai = f"{base_url.rstrip('/')}/v1/models"
                content = await fetch_safely(client, url_openai)

                if content:
                    try:
                        data = json.loads(content)
                        models = []
                        if "data" in data:
                            models = [m.get("id", "") for m in data["data"]]
                        return {"success": True, "models": models}
                    except json.JSONDecodeError:
                        pass

                # å‚™ç”¨å˜—è©¦: Ollama API (/api/tags)
                url_ollama = f"{base_url.rstrip('/')}/api/tags"
                content = await fetch_safely(client, url_ollama)

                if content:
                    try:
                        data = json.loads(content)
                        models = []
                        if "models" in data:
                            models = [m.get("name", "") for m in data["models"]]
                        elif "data" in data:
                            models = [m.get("id", "") for m in data["data"]]
                        return {"success": True, "models": models}
                    except json.JSONDecodeError:
                        pass

                return {
                    "success": False,
                    "error": "ç„¡æ³•é€£æ¥æˆ–æœå‹™å›æ‡‰æ ¼å¼ä¸æ­£ç¢º (åƒ…æ”¯æ´ JSON)",
                }

        except Exception as e2:
            return {"success": False, "error": str(e2)}


def get_ai_service(provider: str, **kwargs) -> AIService:
    """å–å¾— AI æœå‹™å¯¦ä¾‹"""
    if provider == "gemini":
        api_key = kwargs.get("api_key")
        model = kwargs.get("model")
        if not api_key:
            raise ValueError("Gemini API Key æœªæä¾›")
        return GeminiService(api_key, model=model)

    elif provider == "openai":
        api_key = kwargs.get("api_key")
        model = kwargs.get("model")
        if not api_key:
            raise ValueError("OpenAI API Key æœªæä¾›")
        return OpenAIService(api_key, model=model)

    elif provider == "local" or provider == "custom":
        base_url = kwargs.get("base_url") or kwargs.get("local_url")
        model = kwargs.get("model") or kwargs.get("local_model")
        api_key = kwargs.get("api_key")  # Optional

        if not base_url or not model:
            raise ValueError("è‡ªå®šç¾© AI URL æˆ–æ¨¡å‹æœªæä¾›")
        return CustomAIService(base_url, model, api_key)

    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ AI æä¾›è€…: {provider}")
